{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue Trend Analysis and Issue Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, pickle\n",
    "import utils\n",
    "from utils import Corpus, Issue, Extractor, IssueModel, EventModel\n",
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "years = [2015, 2016, 2017]\n",
    "num_issues = 50\n",
    "num_events = 50\n",
    "num_keywords = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean articles: lemmatize, remove stopwords (Already Done)\n",
    "**_Caution!_ Involves multiprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.clean_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Entities: IBM Watson NLU (Already Done)\n",
    "**_Caution!_ Involves multiprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.build_watson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Issue Trend Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for year in years:\n",
    "    corpus[year] = Corpus(year=year-2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Corpuses: Load cleaned articles, build phrasers, dictionary, and BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2015:\n",
      "building corpus...\n",
      "collecting articles...\n",
      "building phrasers...\n",
      "bigram train finished! 7.40 seconds\n",
      "trigram train finished! 12.16 seconds\n",
      "building dictionary...\n",
      "building bag of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7155/7155 [00:03<00:00, 2038.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2015 Done\n",
      "\n",
      "Corpus 2016:\n",
      "building corpus...\n",
      "collecting articles...\n",
      "building phrasers...\n",
      "bigram train finished! 7.98 seconds\n",
      "trigram train finished! 12.99 seconds\n",
      "building dictionary...\n",
      "building bag of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7480/7480 [00:03<00:00, 1881.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2016 Done\n",
      "\n",
      "Corpus 2017:\n",
      "building corpus...\n",
      "collecting articles...\n",
      "building phrasers...\n",
      "bigram train finished! 8.76 seconds\n",
      "trigram train finished! 14.80 seconds\n",
      "building dictionary...\n",
      "building bag of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9117/9117 [00:04<00:00, 2159.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2017 Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    print(\"Corpus \"+str(year)+\":\")\n",
    "    corpus[year].build_corpus()\n",
    "    print(\"Corpus \"+str(year)+\" Done\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Keywords from each Article using tf-ifd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2015:\n",
      "building tf-idf model...\n",
      "tfidf finished! 0.07 seconds\n",
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7155/7155 [00:04<00:00, 1682.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2015 Done\n",
      "\n",
      "Corpus 2016:\n",
      "building tf-idf model...\n",
      "tfidf finished! 0.07 seconds\n",
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7480/7480 [00:04<00:00, 1680.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2016 Done\n",
      "\n",
      "Corpus 2017:\n",
      "building tf-idf model...\n",
      "tfidf finished! 0.08 seconds\n",
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9117/9117 [00:05<00:00, 1712.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2017 Done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    print(\"Corpus \"+str(year)+\":\")\n",
    "    corpus[year].build_tfidf()\n",
    "    corpus[year].extractor = Extractor(corpus[year])\n",
    "    corpus[year].extractor.extract(k=num_keywords)\n",
    "    print(\"Corpus \"+str(year)+\" Done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving corpus_ready.bin...\n",
      "corpus_ready.bin saved\n"
     ]
    }
   ],
   "source": [
    "utils.save(corpus, filename='corpus_ready.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus_ready.bin...\n",
      "corpus_ready.bin loaded\n"
     ]
    }
   ],
   "source": [
    "corpus = utils.load(filename='corpus_ready.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LDA model, cluster articles into issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2015:\n",
      "building LDA model...\n",
      "LDA finished! 8.57 seconds\n",
      "building issues...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7155/7155 [00:05<00:00, 1238.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 2583.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2015 Done\n",
      "\n",
      "Corpus 2016:\n",
      "building LDA model...\n",
      "LDA finished! 9.25 seconds\n",
      "building issues...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7480/7480 [00:06<00:00, 1215.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 3022.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2016 Done\n",
      "\n",
      "Corpus 2017:\n",
      "building LDA model...\n",
      "LDA finished! 10.00 seconds\n",
      "building issues...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9117/9117 [00:07<00:00, 1292.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 998.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 2017 Done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    print(\"Corpus \"+str(year)+\":\")\n",
    "    corpus[year].build_lda(num_topics=num_issues)\n",
    "    corpus[year].issue_model = IssueModel(corpus=corpus[year], model=corpus[year].lda)\n",
    "    corpus[year].issue_model.build_issues()\n",
    "    print(\"Corpus \"+str(year)+\" Done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving corpus_done.bin...\n",
      "corpus_done.bin saved\n"
     ]
    }
   ],
   "source": [
    "utils.save(corpus, filename='corpus_done.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Issues (for Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = []\n",
    "for year in years:\n",
    "    issue_model = corpus[year].issue_model\n",
    "    top_issue_id = issue_model.sorted_issues[0][0]\n",
    "    issues.append(Issue(articles=issue_model.issues[top_issue_id], keywords=issue_model.keywords[top_issue_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Issues (for Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving issues_init.bin...\n",
      "issues_init.bin saved\n"
     ]
    }
   ],
   "source": [
    "utils.save(issues, filename='issues_init.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus_done.bin...\n",
      "corpus_done.bin loaded\n"
     ]
    }
   ],
   "source": [
    "corpus = utils.load(filename='corpus_done.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select year to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_year = 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show top trending issues  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:   5 Score: 1109.61 N:  539 Keywords:  Saenuri, bill, impeachment, Moon, Ahn, Chung, Minjoo, Minjoo_Party, constituency, Opposition_party\n",
      "ID:  14 Score: 946.36 N:  275 Keywords:  N._Korea, sanction, N_K, S._Korea, human_right, Obama, US, NK, resolution, White_House\n",
      "ID:   1 Score: 767.23 N:  187 Keywords:  Trump, comfort_woman, Japan, foundation, S._Korea, sex_slavery, Iran, victim, sexual_slavery, sex_slave\n",
      "ID:  19 Score: 647.80 N:  103 Keywords:  S._Korea, deploy, N_K, nuclear_armament, extended_deterrence, nuclear_weapon, Taurus_missile, military_GPS, exercise, deterrence\n",
      "ID:  45 Score: 593.15 N:  153 Keywords:  N_K, N._Korea, submarine, site, nuke_test, SLBM_test, SLBM, drill, nuclear_warhead, provocation\n",
      "ID:   7 Score: 588.85 N:   99 Keywords:  complex, N_K, park, S._Korea, nuclear_envoy, Yun, N._Korea, nuke_test, Hwang, talk\n",
      "ID:  28 Score: 524.26 N:   50 Keywords:  murder, Patterson, defendant, stab, student, arrest, baby, suspect, fortress, Hwaseong_Fortress\n",
      "ID:  47 Score: 514.92 N:  102 Keywords:  survey, household, proportion, men, Koreans, student, child, marriage, expectancy, study\n",
      "ID:  37 Score: 492.78 N:   43 Keywords:  child_abuse, parent, Police, child, offender, girl, Goseong, mother, teacher, abuse\n",
      "ID:  40 Score: 467.91 N:   48 Keywords:  emission, worker, guideline, Volkswagen_Korea, vehicle, operator, huge_loss, tour, recall, union\n"
     ]
    }
   ],
   "source": [
    "corpus[show_year].issue_model.show_top_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Articles from Top Issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:   5 Score: 1109.61 N:  539 Keywords:  Saenuri, bill, impeachment, Moon, Ahn, Chung, Minjoo, Minjoo_Party, constituency, Opposition_party\n",
      "\t 0 \t Former leader quits opposition party\n",
      "\t 1 \t [Newsmaker] Park’s lame duck deadline looms\n",
      "\t 2 \t Constitutional reform debate resurfaces\n",
      "\t 3 \t General elections mired in uncertainty without constituencies\n",
      "\t 4 \t Park asks political parties to embrace reform\n",
      "ID:  14 Score: 946.36 N:  275 Keywords:  N._Korea, sanction, N_K, S._Korea, human_right, Obama, US, NK, resolution, White_House\n",
      "\t 0 \t Quake in North Korea suspected to be 'explosion': report\n",
      "\t 1 \t North Korea announces successful test of hydrogen bomb\n",
      "\t 2 \t China party paper urges North Korea to change 'nuclear path'\n",
      "\t 3 \t Park, Obama talk over N.K. nuke test\n",
      "\t 4 \t Park, Obama agree to closely work together to adopt strong U.N. sanctions against North\n",
      "ID:   1 Score: 767.23 N:  187 Keywords:  Trump, comfort_woman, Japan, foundation, S._Korea, sex_slavery, Iran, victim, sexual_slavery, sex_slave\n",
      "\t 0 \t Korean-American group to campaign for 'comfort women' issue's\n",
      "\t 1 \t U.N. chief commends Park on Korea-Japan deal of wartime sexual slavery\n",
      "\t 2 \t South Korea, Japan pushing for high-level economic talks in Jan.: source\n",
      "\t 3 \t Ban blasted for sex slavery deal support\n",
      "\t 4 \t Seoul, Tokyo to hold high-level economic talks this month\n",
      "ID:  19 Score: 647.80 N:  103 Keywords:  S._Korea, deploy, N_K, nuclear_armament, extended_deterrence, nuclear_weapon, Taurus_missile, military_GPS, exercise, deterrence\n",
      "\t 0 \t N.K.’s ‘H-bomb test’ claims catches Seoul off guard\n",
      "\t 1 \t N.K. test catches Seoul off guard\n",
      "\t 2 \t Republicans lambast Obama's policy on North Korea after Pyongyang's H-bomb claims\n",
      "\t 3 \t U.S. assures 'ironclad' commitment to defending Korea\n",
      "\t 4 \t U.S. remains committed to extended deterrence against N.K.\n",
      "ID:  45 Score: 593.15 N:  153 Keywords:  N_K, N._Korea, submarine, site, nuke_test, SLBM_test, SLBM, drill, nuclear_warhead, provocation\n",
      "\t 0 \t North may be readying for thermonuclear weapon tests: South Korean military\n",
      "\t 1 \t North Korean leader inspects firing test by military units\n",
      "\t 2 \t North Korea successfully conducts SLBM test last month: U.S. report\n",
      "\t 3 \t North Korea test-fired SLBM last month: South Korean military\n",
      "\t 4 \t South Korea on alert after North's H-bomb test\n",
      "ID:   7 Score: 588.85 N:   99 Keywords:  complex, N_K, park, S._Korea, nuclear_envoy, Yun, N._Korea, nuke_test, Hwang, talk\n",
      "\t 0 \t U.S. asked N. Korea's chief nuclear envoy to meet S. Korean counterpart after 2012 deal: Clinton email\n",
      "\t 1 \t South Korea eyes vice FM talks with U.S., Japan\n",
      "\t 2 \t Ex-Olympic weightlifting champion questioned over assault charges\n",
      "\t 3 \t U.S. says North Korea will be judged by 'actions,' not 'words'\n",
      "\t 4 \t Ex-U.S. Amb. Stephen Bosworth dies\n",
      "ID:  28 Score: 524.26 N:   50 Keywords:  murder, Patterson, defendant, stab, student, arrest, baby, suspect, fortress, Hwaseong_Fortress\n",
      "\t 0 \t Ambulances to be banned from using sirens in non-urgent situations\n",
      "\t 1 \t Lippert sends out New Year's cards to officials\n",
      "\t 2 \t Former USFK official indicted over drug smuggling\n",
      "\t 3 \t Gifted education to help students reach their full potential\n",
      "\t 4 \t Diverse events to mark 220th anniversary of Hwaseong Fortress\n",
      "ID:  47 Score: 514.92 N:  102 Keywords:  survey, household, proportion, men, Koreans, student, child, marriage, expectancy, study\n",
      "\t 0 \t Marriage stress may fuel Korea’s low fertility rate: study\n",
      "\t 1 \t Kerry mourns passing of former Amb. Bosworth\n",
      "\t 2 \t Koreans’ health at risk for sitting too much\n",
      "\t 3 \t Few male Seoulites use paternity leave: report\n",
      "\t 4 \t Koreans stress most about finances\n",
      "ID:  37 Score: 492.78 N:   43 Keywords:  child_abuse, parent, Police, child, offender, girl, Goseong, mother, teacher, abuse\n",
      "\t 0 \t [BEST BRAND] Korea International School committed to 21st century learning\n",
      "\t 1 \t Cold spell eases up in Korea\n",
      "\t 2 \t Ahn apologizes to DJ’s hospitalized widow\n",
      "\t 3 \t [Newsmaker] Kim draws fire over migration, voting rules\n",
      "\t 4 \t Dissatisfaction with parenting leads to dependency on cell phones\n",
      "ID:  40 Score: 467.91 N:   48 Keywords:  emission, worker, guideline, Volkswagen_Korea, vehicle, operator, huge_loss, tour, recall, union\n",
      "\t 0 \t Camping operator to compensate for negligence over death of drowned student\n",
      "\t 1 \t Camping operator to compensate for negligence over death of drowned student\n",
      "\t 2 \t 6 killed after 2 cars fall into sea\n",
      "\t 3 \t Education chiefs propose child care standoff talks\n",
      "\t 4 \t Teachers of institute for disabled, ex-chief jailed for abuse, embezzlement\n"
     ]
    }
   ],
   "source": [
    "corpus[show_year].issue_model.show_issues(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving issues_lda_data.bin...\n",
      "issues_lda_data.bin saved\n"
     ]
    }
   ],
   "source": [
    "issues_lda_data = pyLDAvis.gensim.prepare(corpus[show_year].lda, corpus[show_year].get_bows(), corpus[show_year].dict)\n",
    "utils.save(issues_lda_data, 'issues_lda_data.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.show(data=data, ip='143.248.137.26', port=9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Issue Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, pickle\n",
    "import utils\n",
    "from utils import Corpus, Issue, Extractor, IssueModel, EventModel\n",
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "years = [2015, 2016, 2017]\n",
    "num_issues = 50\n",
    "num_events = 50\n",
    "num_keywords = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading issues_init.bin...\n",
      "issues_init.bin loaded\n"
     ]
    }
   ],
   "source": [
    "issues = utils.load('issues_init.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1:\n",
      "building issue...\n",
      "building phrasers...\n",
      "bigram train finished! 0.30 seconds\n",
      "trigram train finished! 0.48 seconds\n",
      "building dictionary...\n",
      "building bag of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 1848.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1 Done\n",
      "\n",
      "Issue 2:\n",
      "building issue...\n",
      "building phrasers...\n",
      "bigram train finished! 0.79 seconds\n",
      "trigram train finished! 1.19 seconds\n",
      "building dictionary...\n",
      "building bag of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539/539 [00:00<00:00, 1718.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 2 Done\n",
      "\n",
      "Issue 3:\n",
      "building issue...\n",
      "building phrasers...\n",
      "bigram train finished! 0.94 seconds\n",
      "trigram train finished! 1.41 seconds\n",
      "building dictionary...\n",
      "building bag of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:00<00:00, 1739.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 3 Done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, issue in enumerate(issues):\n",
    "    print(\"Issue \"+str(i+1)+\":\")\n",
    "    issue.build_issue()\n",
    "    print(\"Issue \"+str(i+1)+\" Done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract keywords from each Article using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1:\n",
      "building tf-idf model...\n",
      "tfidf finished! 0.01 seconds\n",
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 2054.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1 Done\n",
      "\n",
      "Issue 2:\n",
      "building tf-idf model...\n",
      "tfidf finished! 0.01 seconds\n",
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539/539 [00:00<00:00, 1287.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 2 Done\n",
      "\n",
      "Issue 3:\n",
      "building tf-idf model...\n",
      "tfidf finished! 0.01 seconds\n",
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:00<00:00, 1573.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 3 Done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, issue in enumerate(issues):\n",
    "    print(\"Issue \"+str(i+1)+\":\")\n",
    "    issue.build_tfidf()\n",
    "    issue.extractor = Extractor(issue)\n",
    "    issue.extractor.extract(k=num_keywords)\n",
    "    print(\"Issue \"+str(i+1)+\" Done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving issues_ready.bin...\n",
      "issues_ready.bin saved\n"
     ]
    }
   ],
   "source": [
    "utils.save(issues, filename='issues_ready.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading issues_ready.bin...\n",
      "issues_ready.bin loaded\n"
     ]
    }
   ],
   "source": [
    "issues = utils.load(filename='issues_ready.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LDA model, cluster articles into events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1:\n",
      "building LDA model...\n",
      "LDA finished! 0.67 seconds\n",
      "building events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 1124.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 8466.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1 Done\n",
      "\n",
      "Issue 2:\n",
      "building LDA model...\n",
      "LDA finished! 1.20 seconds\n",
      "building events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539/539 [00:00<00:00, 908.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 7778.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 2 Done\n",
      "\n",
      "Issue 3:\n",
      "building LDA model...\n",
      "LDA finished! 1.51 seconds\n",
      "building events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 744/744 [00:00<00:00, 1097.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 7536.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 3 Done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, issue in enumerate(issues):\n",
    "    print(\"Issue \"+str(i+1)+\":\")\n",
    "    issue.build_lda(num_topics=num_events)\n",
    "    issue.event_model = EventModel(issue=issue, model=issue.lda)\n",
    "    issue.event_model.build_events(threshold=0.5)\n",
    "    print(\"Issue \"+str(i+1)+\" Done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide events into set of independent events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1:\n",
      "building event times...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1769.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building event details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 845.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1 Done\n",
      "\n",
      "Issue 2:\n",
      "building event times...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 4032.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building event details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 408.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 2 Done\n",
      "\n",
      "Issue 3:\n",
      "building event times...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1089.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building event details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 902.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 3 Done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "threshold=[0.40, 0.40, 0.40]\n",
    "for i, issue in enumerate(issues):\n",
    "    print(\"Issue \"+str(i+1)+\":\")\n",
    "    issue.event_model.build_independents(threshold=threshold[i])\n",
    "    issue.event_model.filter_events(k=5)\n",
    "    issue.event_model.build_event_times()\n",
    "    issue.event_model.build_sorted_independents()\n",
    "    issue.event_model.build_event_details()\n",
    "    print(\"Issue \"+str(i+1)+\" Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue 1:\n",
      "[[28], [44], [18], [26], [19]]\n",
      "Issue 1 Done\n",
      "\n",
      "Issue 2:\n",
      "[[49], [44, 18], [9], [31]]\n",
      "Issue 2 Done\n",
      "\n",
      "Issue 3:\n",
      "[[0], [1], [42, 44], [33]]\n",
      "Issue 3 Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, issue in enumerate(issues):\n",
    "    print(\"Issue \"+str(i+1)+\":\")\n",
    "    print(issue.event_model.sorted_independents)\n",
    "    print(\"Issue \"+str(i+1)+\" Done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving issues_done.bin...\n",
      "issues_done.bin saved\n"
     ]
    }
   ],
   "source": [
    "utils.save(issues, 'issues_done.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading issues_done.bin...\n",
      "issues_done.bin loaded\n"
     ]
    }
   ],
   "source": [
    "issues = utils.load('issues_done.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Issue Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump, N._Korea, S._Korea, Tillerson, NK, Mattis, THAAD, White_House, dialogue, envoy\n"
     ]
    }
   ],
   "source": [
    "print(', '.join([keyword[0] for keyword in issues[i].keywords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Top Events  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:   1 Score: 137.92 N:   72 Keywords:  Pence, Hwang, Mike_Pence, Vice_President, summit_Xi, proposal, S._Korea, expert, father, US-China\n",
      "ID:   0 Score:  68.38 N:   36 Keywords:  prepare, Tillerson, Xi, totally, Americans, offer_talk, envoy, President_Xi, Hong, special_envoy\n",
      "ID:  33 Score:  60.47 N:   36 Keywords:  Mattis, S._Korea, Song, Military, Nauert, State_Department, US_senator, Corker, Bob_Corker, senator\n",
      "ID:  44 Score:  54.11 N:   37 Keywords:  S._Korea, n't_think, Mattis, Song_Mattis, envoy, Kang, North_Koreans, visit_DMZ, Korean, diplomatic_effort\n",
      "ID:  42 Score:  53.76 N:   29 Keywords:  S._Korea, Choi, transparency, fix, professor, scandal, defense_spending, unity_N., Hwang, N._Korean\n",
      "ID:  40 Score:  49.06 N:   27 Keywords:  NK_provocation, Kang, Korea-US_alliance, Tillerson, Chung, channel, Russia, FTA, Yun, objective\n",
      "ID:  19 Score:  40.05 N:   26 Keywords:  Moon_Abe, level, Mattis, South_Korean_leader, increase_pressure, Shinsuke_Sugiyama, envoy, S._Korea, project, trilateral_economic\n",
      "ID:  45 Score:  36.63 N:   20 Keywords:  isolate, interview_CBS, deterrence, UN_sanction, Cheong, accord_Park, payload, ferry, Shanghai_Salvage, Xi\n",
      "ID:  10 Score:  34.11 N:   23 Keywords:  THAAD_retaliation, retaliatory_measure, slam_China, unreasonable, necessary_step, Rep., self-defense, Democratic_Party, party, Seoul_Washington\n",
      "ID:  24 Score:  33.99 N:   19 Keywords:  envoy, Cho, nuke, Cho_Yoon-je, S._Korea, Senator, Graham, dependent, family, Lindsey_Graham\n"
     ]
    }
   ],
   "source": [
    "issues[i].event_model.show_top_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Articles from Top Events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:   1 Score: 137.92 N:   72 Keywords:  Pence, Hwang, Mike_Pence, Vice_President, summit_Xi, proposal, S._Korea, expert, father, US-China\n",
      "\t 0 \t Korea mulls disclosing defense spending to deflect US' calls for burden sharing\n",
      "\t 1 \t [HERALD INTERVIEW] ‘US-China clash could reset inter-Korean ties’\n",
      "\t 2 \t Trump says ‘100 percent’ with Seoul on NK\n",
      "\t 3 \t Tillerson says Korea already paying 'large amounts' for US troops\n",
      "\t 4 \t Trump reaffirms US security commitment to Japan after N.K. missile launch\n",
      "ID:   0 Score:  68.38 N:   36 Keywords:  prepare, Tillerson, Xi, totally, Americans, offer_talk, envoy, President_Xi, Hong, special_envoy\n",
      "\t 0 \t Tillerson urges China to 'use all available tools' to rein in N. Korea\n",
      "\t 1 \t N. Korea, THAAD key topics for Trump's summit with Xi: White House\n",
      "\t 2 \t Trump again calls NK problem ‘mess’\n",
      "\t 3 \t Trump to host China's Xi at Florida resort next week\n",
      "\t 4 \t [Herald Interview] ‘US not afraid to strike North Korea in war’\n",
      "ID:  33 Score:  60.47 N:   36 Keywords:  Mattis, S._Korea, Song, Military, Nauert, State_Department, US_senator, Corker, Bob_Corker, senator\n",
      "\t 0 \t Kerry warns of 'forceful ways' against N. Korea\n",
      "\t 1 \t N. Korea using sensors along border to clamp down on defectors\n",
      "\t 2 \t Resumption of Kaesong complex to be hotly debated amid sanctions regime\n",
      "\t 3 \t Top diplomats betray US-China differences on how to rein in N. Korea\n",
      "\t 4 \t US congressman calls for preparedness for pre-emptive strike on N. Korea\n",
      "ID:  44 Score:  54.11 N:   37 Keywords:  S._Korea, n't_think, Mattis, Song_Mattis, envoy, Kang, North_Koreans, visit_DMZ, Korean, diplomatic_effort\n",
      "\t 0 \t S. Korea condemns NK's war threats over Seoul-Washington military drill\n",
      "\t 1 \t Mattis sees visit to S. Korea as 'listening trip': source\n",
      "\t 2 \t Trump calls N. Korea 'world menace' that should be dealt with 'soon'\n",
      "\t 3 \t US must ensure no daylight with S. Korea's next leader over N. Korea: US expert\n",
      "\t 4 \t Tillerson to visit DMZ as part of Korean tour\n",
      "ID:  42 Score:  53.76 N:   29 Keywords:  S._Korea, Choi, transparency, fix, professor, scandal, defense_spending, unity_N., Hwang, N._Korean\n",
      "\t 0 \t Chinese FM calls for S. Korea to halt THAAD deployment\n",
      "\t 1 \t Acting president calls for continued 'across-the-board pressure' to induce NK denuclearization\n",
      "\t 2 \t No major changes to US-NK ties under Trump: professor\n",
      "\t 3 \t 'Korea should avoid getting stuck between US, China'\n",
      "\t 4 \t Trump reiterates 'ironclad commitment' to defend Korea\n",
      "ID:  40 Score:  49.06 N:   27 Keywords:  NK_provocation, Kang, Korea-US_alliance, Tillerson, Chung, channel, Russia, FTA, Yun, objective\n",
      "\t 0 \t US official warns NK provocations could affect Trump's strategy on Pyongyang\n",
      "\t 1 \t Mattis urges NATO allies to meet burden-sharing obligations\n",
      "\t 2 \t Tillerson: US can consider dialogue after N. Korea stops weapons testing\n",
      "\t 3 \t Regime change not US goal: Tillerson\n",
      "\t 4 \t US will not bypass Seoul when dealing with N. Korea: FM\n",
      "ID:  19 Score:  40.05 N:   26 Keywords:  Moon_Abe, level, Mattis, South_Korean_leader, increase_pressure, Shinsuke_Sugiyama, envoy, S._Korea, project, trilateral_economic\n",
      "\t 0 \t S. Korea, US, Japan agree to accelerate efforts to increase pressure on N. Korea\n",
      "\t 1 \t N. Korea broadcasts new coded numbers for spies\n",
      "\t 2 \t S. Korea, Japan, China to discuss N. Korea's cyber threats in Tokyo this week\n",
      "\t 3 \t Top diplomats of S. Korea, US to discuss NK nukes in Germany\n",
      "\t 4 \t Gov't to monitor cyberattacks utilizing IoT devices\n",
      "ID:  45 Score:  36.63 N:   20 Keywords:  isolate, interview_CBS, deterrence, UN_sanction, Cheong, accord_Park, payload, ferry, Shanghai_Salvage, Xi\n",
      "\t 0 \t Korea, US, Japan to hold vice foreign ministers' talks in Washington\n",
      "\t 1 \t Moon expresses misgivings about possible military action on peninsula\n",
      "\t 2 \t Moon to add dialogue to sanctions in dealing with N. Korea\n",
      "\t 3 \t Moon says NK will only has isolation to gain from missile launches\n",
      "\t 4 \t S. Korean president calls for deterrence against N. Korea's missile threats\n",
      "ID:  10 Score:  34.11 N:   23 Keywords:  THAAD_retaliation, retaliatory_measure, slam_China, unreasonable, necessary_step, Rep., self-defense, Democratic_Party, party, Seoul_Washington\n",
      "\t 0 \t Seoul, Washington slam China’s THAAD retaliation as ‘unreasonable’\n",
      "\t 1 \t Seoul, Washington slam China over THAAD retaliation\n",
      "\t 2 \t China vows to faithfully carry out sanctions on N. Korea, work on denuclearization\n",
      "\t 3 \t Tillerson to face thorny issues over two Koreas, China\n",
      "\t 4 \t Korea considers bringing China's THAAD retaliation to WTO\n",
      "ID:  24 Score:  33.99 N:   19 Keywords:  envoy, Cho, nuke, Cho_Yoon-je, S._Korea, Senator, Graham, dependent, family, Lindsey_Graham\n",
      "\t 0 \t Biden: N. Korea will pose significant challenge to Trump administration\n",
      "\t 1 \t Diplomat says N. Korea not interested in denuclearization dialogue\n",
      "\t 2 \t White House: US strikes on Syria show Trump's policy on N. Korea will be different\n",
      "\t 3 \t Tillerson to chair special UNSC meeting on NK\n",
      "\t 4 \t Trump’s crude, unilateral approach threatens to undercut alliance\n"
     ]
    }
   ],
   "source": [
    "issues[i].event_model.show_events(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Issue Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Keywords: \n",
      "\t Trump, N._Korea, S._Korea, Tillerson, NK, Mattis, THAAD, White_House, dialogue, envoy\n",
      "\n",
      "Events: \n",
      "\t 0\n",
      "\t 1\n",
      "\t 42 -> 44\n",
      "\t 33\n",
      "\n",
      "\n",
      "Event ID:   0\n",
      "Event Keywords: \n",
      "\t prepare, Tillerson, Xi, totally, Americans, offer_talk, envoy, President_Xi, Hong, special_envoy\n",
      "Time: 2017-08-01\n",
      "Entities: \n",
      "\tNorth Korea, LOCATION\n",
      "\tUS, LOCATION\n",
      "\tDonald Trump, PERSON\n",
      "\tChina, LOCATION\n",
      "\tSouth Korea, LOCATION\n",
      "\tPyongyang, LOCATION\n",
      "\tPresident Donald Trump, PERSON\n",
      "\tPresident, JOBTITLE\n",
      "\tpresident, JOBTITLE\n",
      "\tBeijing, LOCATION\n",
      "\n",
      "Event ID:   1\n",
      "Event Keywords: \n",
      "\t Pence, Hwang, Mike_Pence, Vice_President, summit_Xi, proposal, S._Korea, expert, father, US-China\n",
      "Time: 2017-06-25\n",
      "Entities: \n",
      "\tNorth Korea, LOCATION\n",
      "\tSouth Korea, LOCATION\n",
      "\tUS, LOCATION\n",
      "\tSeoul, LOCATION\n",
      "\tDonald Trump, PERSON\n",
      "\tChina, LOCATION\n",
      "\tPyongyang, LOCATION\n",
      "\tPresident Donald Trump, PERSON\n",
      "\tPresident, JOBTITLE\n",
      "\tWashington, LOCATION\n",
      "\n",
      "Event ID:  42\n",
      "Event Keywords: \n",
      "\t S._Korea, Choi, transparency, fix, professor, scandal, defense_spending, unity_N., Hwang, N._Korean\n",
      "Time: 2017-06-15\n",
      "Entities: \n",
      "\tNorth Korea, LOCATION\n",
      "\tSouth Korea, LOCATION\n",
      "\tUS, LOCATION\n",
      "\tChina, LOCATION\n",
      "\tSeoul, LOCATION\n",
      "\tPyongyang, LOCATION\n",
      "\tDonald Trump, PERSON\n",
      "\tUnited States, LOCATION\n",
      "\tKorean Peninsula, GEOGRAPHICFEATURE\n",
      "\tWashington, LOCATION\n",
      "\n",
      "Event ID:  44\n",
      "Event Keywords: \n",
      "\t S._Korea, n't_think, Mattis, Song_Mattis, envoy, Kang, North_Koreans, visit_DMZ, Korean, diplomatic_effort\n",
      "Time: 2017-07-22\n",
      "Entities: \n",
      "\tNorth Korea, LOCATION\n",
      "\tSouth Korea, LOCATION\n",
      "\tUS, LOCATION\n",
      "\tDonald Trump, PERSON\n",
      "\tUnited States, LOCATION\n",
      "\tSeoul, LOCATION\n",
      "\tPyongyang, LOCATION\n",
      "\tPresident Donald Trump, PERSON\n",
      "\tKorean Peninsula, GEOGRAPHICFEATURE\n",
      "\tWashington, LOCATION\n",
      "\n",
      "Event ID:  33\n",
      "Event Keywords: \n",
      "\t Mattis, S._Korea, Song, Military, Nauert, State_Department, US_senator, Corker, Bob_Corker, senator\n",
      "Time: 2017-07-11\n",
      "Entities: \n",
      "\tNorth Korea, LOCATION\n",
      "\tSouth Korea, LOCATION\n",
      "\tChina, LOCATION\n",
      "\tUnited States, LOCATION\n",
      "\tPyongyang, LOCATION\n",
      "\tUS, LOCATION\n",
      "\tJim Mattis, PERSON\n",
      "\tPresident Donald Trump, PERSON\n",
      "\tpresident, JOBTITLE\n",
      "\tWASHINGTON, LOCATION\n"
     ]
    }
   ],
   "source": [
    "issues[i].event_model.show_issue_summary(num_entities=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving events_lda_data.bin...\n",
      "events_lda_data.bin saved\n"
     ]
    }
   ],
   "source": [
    "events_lda_data = pyLDAvis.gensim.prepare(issues[i].lda, issues[i].get_bows(), issues[i].dict)\n",
    "utils.save(events_lda_data, 'events_lda_data.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.show(data=data, ip='143.248.137.26', port=9000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
